---
title: "hw3"
author: "Ruwen Zhou"
date: "10/8/2020"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rnoaa)
library(ggridges)
library(patchwork)

library(p8105.datasets)
knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Problem 1

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. 

Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles, and which are most items from?

```{r}
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```

doc plot
```{r}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

plot table

```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```

apple vs ice cream

```{r}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)
```

# Problem 2

##Load and clean data

```{r}
accel = 
  read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "number_of_activity") %>% 
  drop_na("number_of_activity") %>% 
  mutate(
    minute = as.numeric(minute),
    number_of_activity = as.numeric(number_of_activity),
    week = as.character(week),
    day = as.factor(day),
    day_id = as.factor(day_id) 
  ) %>% 
  mutate(weekend = day %in% c("Sunday","Saturday"),
         weekday = day %in% c("Monday","Tuesday", "Wednesday","Thursday","Friday"),
         weekend_vs_weekday=
           case_when(weekend ~"weekend",
                     weekday ~ "weekday")) %>% 
  mutate(weekend_vs_weekday = as.factor(weekend_vs_weekday),
         day = forcats::fct_relevel(day,c("Monday","Tuesday","Wednesday","Thursday",
                                     "Friday", "Saturday","Sunday"))) %>% 
  group_by(week,day) %>% 
  arrange(day,.by_group = T) %>% 
  relocate(day_id, week, weekend_vs_weekday)

accel
```
First, we read data from "accel_data.csv" and clean it. Since the data in the excel is in a format that hard to read, I used pivot_longer and count the number of activities of different time length. And then drop null values and mutate the format of some variables for calculation. Then we create weekend_vs_weekday column. The last step is to group, relocate the data and make it more readable. Finally, this dataset has `r nrow(accel)` rows and `r ncol(accel)` columns, with variables day_id, week, weekend_vs_weekday, day, minute, number_of_activity, weekend, weekday.

## Traditinal analysis
```{r}
accel %>% 
  summarize(total_activity = sum(number_of_activity)) %>% 
  pivot_wider(names_from = week,
              values_from = total_activity,
              names_prefix = "week") %>% 
  knitr::kable()
```
Trend: People are having significantly less activity on Saturday, especially on week 4 and week 5.

## Single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week.
```{r}
accel
```
```{r}
accel %>% 
  group_by(day_id,day,minute) %>% 
  summarize(total_activity = sum(number_of_activity)) %>%
  ggplot(aes(x = minute, y = total_activity, color = day)) +
  geom_smooth(se = FALSE) +
  labs(
    title = "24-hour activity time counts for each day"
    )+
  scale_y_continuous(trans = "sqrt",
                     name = "Activities",
                     breaks = c(50,100,150,200,250,300,350,400))+
  scale_x_continuous(name = "Hour",
                     breaks = c(60,120,180, 240,300, 360,420, 480,540, 600,660, 720,780, 840,900, 960,1020, 1080,1140, 1200,1260, 1320,1380, 1440),
                     labels = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24))
  
```
Based on this graph, we can see that everyday, activities usually start increasing form 6am. And until around 10am, it reaches its peak. The amount of activity done on evening is usually larger than on the afternoon. On Friday, there is a peak time at 9pm.

# Problem 3

```{r}
library(p8105.datasets)
data("ny_noaa")
```

## Tidy data
```{r}
nynoaa_tidy = ny_noaa %>% 
  mutate_at(vars(prcp, tmax, tmin, snow), as.numeric) %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate_at(vars(month),as.numeric) %>% 
  mutate(month = month.name[month])
skimr::skim_without_charts(nynoaa_tidy)
```
```{r}
sum(is.na(nynoaa_tidy$prcp))/nrow(nynoaa_tidy)
sum(is.na(nynoaa_tidy$snow))/nrow(nynoaa_tidy)
sum(is.na(nynoaa_tidy$snwd))/nrow(nynoaa_tidy)
sum(is.na(nynoaa_tidy$tmax))/nrow(nynoaa_tidy)
sum(is.na(nynoaa_tidy$tmin))/nrow(nynoaa_tidy)
```

This dataset has `r nrow(nynoaa_tidy)` rows and `r ncol(nynoaa_tidy)` columns. It starts from 1981-01-01 to 2010-12-31, containing the weather infomation of these days, including prcp, snow, snwd, tmax, and tmin. In tmax and tmin, the proportion of missing value is up to 43%, which is very high.

## Snow data
```{r}
nynoaa_tidy %>% 
  count(snow, na.rm = T) %>% 
  mutate(rank = min_rank(desc(n)))
```
The most commonly observed value is 0 for snow, since usually there is no snow.

## Make a two-panel plot showing the average max temperature in January and in July in each station across years

###January
```{r}
january=
nynoaa_tidy %>% 
  filter(month == 'January') %>% 
  group_by(id,month,year) %>% 
  summarise(max_mean = mean(tmax, na.rm = TRUE),.groups = 'drop') %>% 
  drop_na() %>% 
  ggplot(aes(x = year,
             y = max_mean,
             color = id,
             group = id))+
  geom_point(alpha = 0.3)+
  geom_path(alpha = 0.3)+
  theme(legend.position = 'none',
        axis.title.x = element_blank())+
  labs(
    x = "Year",
    y = "Temperature",
    title = "Maximun temperature in January of each years"
    )



```
###July
```{r}
july=
nynoaa_tidy %>% 
  filter(month == 'July') %>% 
  group_by(id,month,year) %>% 
  summarise(max_mean = mean(tmax, na.rm = TRUE),.groups = 'drop') %>% 
  drop_na() %>% 
  ggplot(aes(x = year,
             y = max_mean,
             color = id,
             group = id))+
  geom_point(alpha = 0.3)+
  geom_path(alpha = 0.3)+
  theme(legend.position = 'none',
        axis.title.x = element_blank())+
  labs(
    x = "Year",
    y = "Temperature",
    title = "Maximun temperature in July of each years"
    )
```

```{r}
january/july
```
* Overall temperature in July is higher than in January in NYC. Temperature in January is around -100 to 100, and in July, it is from around 200 to 300.
* There are many outliers for both datasets. For example, 1982 and 2005 had much lower temperature on January, 1988 had much lower temperature on July.

##Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option)

```{r}
tmax_vs_tmin = 
  nynoaa_tidy %>% 
  drop_na(tmax, tmin) %>% 
  pivot_longer(
    tmax:tmin,
    names_to = "tmax_vs_tmin",
    values_to = "temperature"
  ) %>% 
  ggplot(aes(x = year, y = temperature)) +
  geom_boxplot(aes(color = tmax_vs_tmin), alpha = 0.3)+

  labs(
    x = "Year",
    y = "Temperature",
    title = "tmax vs tmin"
    )
tmax_vs_tmin
```























